{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex Optimization Experiments\n",
    "\n",
    "This notebook executes the convex optimization experiments from the TRAC_CV codebase. It allows you to run experiments with different optimizers on linear and logistic regression tasks, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Make sure the directory structure is in Python path\n",
    "import sys\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Local Modules\n",
    "\n",
    "We'll import the necessary modules from the convex_experiments directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import local modules\n",
    "from convex_experiments.models.linear_models import LinearRegressionModel, LogisticRegressionModel\n",
    "from convex_experiments.data.synthetic_data import generate_linear_data, generate_logistic_data\n",
    "from convex_experiments.optimizers.trac_pytorch import start_trac\n",
    "from convex_experiments.trainer import train_convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Settings\n",
    "\n",
    "Define the settings for our experiments. You can modify these parameters to try different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Experiment settings\n",
    "config = {\n",
    "    'num_samples': 1000,         # Number of data samples\n",
    "    'num_features': 20,          # Number of features\n",
    "    'noise_std': 0.1,            # Noise standard deviation for linear regression\n",
    "    'num_iterations': 500,       # Number of training iterations\n",
    "    'seed': 42,                  # Random seed\n",
    "    'log_interval': 10,          # Log metrics every N iterations\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'learning_rates': {          # Learning rates for different optimizers\n",
    "        'SGD': 0.01,\n",
    "        'Adam': 0.01,\n",
    "        'Adagrad': 0.1,\n",
    "        'RMSprop': 0.01,\n",
    "        'TRAC_SGD': 0.01,        # TRAC might adapt/ignore this\n",
    "        'TRAC_Adam': 0.01,       # TRAC might adapt/ignore this\n",
    "    },\n",
    "    'optimizers': ['SGD', 'Adam', 'TRAC_SGD', 'TRAC_Adam'],  # Optimizers to use\n",
    "    'tasks': ['linear', 'logistic'],                        # Tasks to run\n",
    "}\n",
    "\n",
    "# Create logs directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f'convex_experiments/logs_{timestamp}'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"Logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds\n",
    "\n",
    "Ensure reproducibility by setting random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    return\n",
    "\n",
    "set_seeds(config['seed'])\n",
    "device = torch.device(config['device'])\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def setup_optimizer(optimizer_name, model, lr):\n",
    "    \"\"\"Set up the specified optimizer.\"\"\"\n",
    "    # TRAC requires a log file\n",
    "    trac_dummy_log = os.path.join(log_dir, f\"trac_internal_{optimizer_name}_{config['seed']}.log\")\n",
    "    if os.path.exists(trac_dummy_log):\n",
    "        os.remove(trac_dummy_log)  # Ensure clean start if re-running\n",
    "    \n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'Adagrad':\n",
    "        return optim.Adagrad(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'TRAC_SGD':\n",
    "        optimizer_class = start_trac(log_file=trac_dummy_log, Base=optim.SGD)\n",
    "        return optimizer_class(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'TRAC_Adam':\n",
    "        optimizer_class = start_trac(log_file=trac_dummy_log, Base=optim.Adam)\n",
    "        return optimizer_class(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "def run_experiment(task, optimizer_name):\n",
    "    \"\"\"Run a single experiment for the given task and optimizer.\"\"\"\n",
    "    print(f\"\\nRunning experiment: Task={task}, Optimizer={optimizer_name}\")\n",
    "    \n",
    "    # Set up data\n",
    "    w_true = None\n",
    "    if task == 'linear':\n",
    "        X, y, w_true = generate_linear_data(\n",
    "            config['num_samples'], \n",
    "            config['num_features'], \n",
    "            config['noise_std'], \n",
    "            config['seed']\n",
    "        )\n",
    "        loss_fn = nn.MSELoss()\n",
    "        model = LinearRegressionModel(config['num_features']).to(device)\n",
    "    elif task == 'logistic':\n",
    "        X, y, w_true = generate_logistic_data(\n",
    "            config['num_samples'], \n",
    "            config['num_features'], \n",
    "            config['seed']\n",
    "        )\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        model = LogisticRegressionModel(config['num_features']).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {task}\")\n",
    "    \n",
    "    # Move data to the appropriate device\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    w_true = w_true.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = setup_optimizer(optimizer_name, model, config['learning_rates'][optimizer_name])\n",
    "    \n",
    "    # Train the model\n",
    "    log_data = train_convex(\n",
    "        model=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        num_iterations=config['num_iterations'],\n",
    "        w_true=w_true,\n",
    "        log_interval=config['log_interval']\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    results_filename = os.path.join(log_dir, f\"results_{task}_{optimizer_name}_seed{config['seed']}.csv\")\n",
    "    log_df.to_csv(results_filename, index=False)\n",
    "    print(f\"Results saved to {results_filename}\")\n",
    "    \n",
    "    return log_df\n",
    "\n",
    "def run_all_experiments():\n",
    "    \"\"\"Run all experiments specified in the config.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for task in config['tasks']:\n",
    "        results[task] = {}\n",
    "        for optimizer in config['optimizers']:\n",
    "            log_df = run_experiment(task, optimizer)\n",
    "            results[task][optimizer] = log_df\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments\n",
    "\n",
    "Now, let's run all the configured experiments. This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run all the experiments\n",
    "experiment_results = run_all_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Define functions to visualize the results of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define consistent colors for optimizers\n",
    "OPTIMIZER_COLORS = {\n",
    "    'SGD': '#1f77b4', \n",
    "    'Adam': '#ff7f0e',\n",
    "    'Adagrad': '#2ca02c',\n",
    "    'RMSprop': '#d62728',\n",
    "    'TRAC_SGD': '#9467bd', \n",
    "    'TRAC_Adam': '#8c564b',\n",
    "}\n",
    "\n",
    "# Metrics to plot (y-axis) and their preferred y-axis labels\n",
    "METRICS_TO_PLOT = {\n",
    "    'loss': 'Loss',\n",
    "    'gradient_norm': 'Gradient Norm',\n",
    "    'distance_sq_to_opt': 'Squared Distance to Optimum',\n",
    "    'trac_s_sum': 'TRAC Sum(s)',\n",
    "}\n",
    "\n",
    "def plot_results_from_dataframes(results, task, x_axis='iteration'):\n",
    "    \"\"\"Generate plots directly from the experiment results dataframes.\"\"\"\n",
    "    # Set seaborn style\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    \n",
    "    # Combine all optimizer results for this task\n",
    "    all_data = []\n",
    "    for optimizer, df in results[task].items():\n",
    "        df_copy = df.copy()\n",
    "        df_copy['optimizer'] = optimizer\n",
    "        all_data.append(df_copy)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Create plots for each metric\n",
    "    for metric, y_label in METRICS_TO_PLOT.items():\n",
    "        if metric not in combined_df.columns:\n",
    "            print(f\"Metric '{metric}' not found in data, skipping plot.\")\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=combined_df,\n",
    "            x=x_axis,\n",
    "            y=metric,\n",
    "            hue='optimizer',\n",
    "            palette=OPTIMIZER_COLORS,\n",
    "            legend='full'\n",
    "        )\n",
    "        \n",
    "        plt.title(f'{task.capitalize()} Task: {y_label} vs. {x_axis.capitalize()}')\n",
    "        plt.xlabel(x_axis.capitalize())\n",
    "        plt.ylabel(y_label)\n",
    "        plt.legend(title='Optimizer')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_filename = os.path.join(log_dir, f\"plot_{task}_{metric}_vs_{x_axis}.png\")\n",
    "        plt.savefig(plot_filename)\n",
    "        print(f\"Plot saved to {plot_filename}\")\n",
    "        plt.show()\n",
    "\n",
    "def plot_all_results(results, x_axis='iteration'):\n",
    "    \"\"\"Generate plots for all tasks in the results.\"\"\"\n",
    "    for task in results.keys():\n",
    "        plot_results_from_dataframes(results, task, x_axis)\n",
    "        \n",
    "def load_results_from_logs(log_dir):\n",
    "    \"\"\"Load experiment results from CSV files in the log directory.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Find all CSV files in the log directory\n",
    "    csv_files = glob.glob(os.path.join(log_dir, \"results_*.csv\"))\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        # Extract task and optimizer from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        \n",
    "        # Assuming format: results_task_optimizer_seedX.csv\n",
    "        task = parts[1]\n",
    "        optimizer = '_'.join(parts[2:-1]) if len(parts) > 3 else parts[2].split('seed')[0]\n",
    "        \n",
    "        # Initialize nested dictionaries if needed\n",
    "        if task not in results:\n",
    "            results[task] = {}\n",
    "            \n",
    "        # Load data\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            results[task][optimizer] = df\n",
    "            print(f\"Loaded results for {task} - {optimizer}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Plots for Experiment Results\n",
    "\n",
    "Now, let's visualize the results of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot all results using iteration as x-axis\n",
    "plot_all_results(experiment_results, x_axis='iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot all results using time as x-axis\n",
    "plot_all_results(experiment_results, x_axis='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Results from CSV Files\n",
    "\n",
    "You can also load and visualize results from previously saved CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If you want to load results from a specific log directory\n",
    "# previous_log_dir = 'convex_experiments/logs_previous_run'\n",
    "# loaded_results = load_results_from_logs(previous_log_dir)\n",
    "# plot_all_results(loaded_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Analysis\n",
    "\n",
    "Let's analyze the performance of the different optimizers on each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Performance analysis function\n",
    "def analyze_performance(results):\n",
    "    \"\"\"Analyze and compare the performance of different optimizers.\"\"\"\n",
    "    summary = {}\n",
    "    \n",
    "    for task in results.keys():\n",
    "        summary[task] = {}\n",
    "        \n",
    "        for optimizer, df in results[task].items():\n",
    "            # Get final values (last row)\n",
    "            final_metrics = df.iloc[-1].to_dict()\n",
    "            \n",
    "            # Extract relevant metrics\n",
    "            summary[task][optimizer] = {\n",
    "                'final_loss': final_metrics.get('loss', float('nan')),\n",
    "                'final_gradient_norm': final_metrics.get('gradient_norm', float('nan')),\n",
    "                'final_distance_sq': final_metrics.get('distance_sq_to_opt', float('nan')),\n",
    "                'total_time': final_metrics.get('time', float('nan')),\n",
    "                'final_iteration': final_metrics.get('iteration', float('nan'))\n",
    "            }\n",
    "    \n",
    "    # Create and display summary dataframes\n",
    "    for task, optimizers_data in summary.items():\n",
    "        print(f\"\\n{task.upper()} TASK SUMMARY:\")\n",
    "        task_df = pd.DataFrame.from_dict(optimizers_data, orient='index')\n",
    "        print(task_df)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Analyze the experiment results\n",
    "performance_summary = analyze_performance(experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated how to run and visualize the convex optimization experiments. You can modify the configuration settings to try different scenarios, such as using different optimizers, changing learning rates, or trying different dataset sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}